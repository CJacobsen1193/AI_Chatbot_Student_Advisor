{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To query the chatbot please follow these steps:\n",
        "1. Install all neccessary requirements\n",
        "2. Get personal huggingface token\n",
        "3. Load Llaama 2 7B Chat HuggingFace model\n",
        "4. Read from stored Qdrant database\n",
        "5. Query the model!"
      ],
      "metadata": {
        "id": "Ce8EJ-ahEjOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs all necessary requirements"
      ],
      "metadata": {
        "id": "Nk_a6nStf2Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run Streamlit:\n",
        "\n",
        "Download requirements below\n",
        "\n",
        "Get .py file from github\n",
        "\n",
        "Run codeblocks under \"Run Streamlight\" at end of notebook\n"
      ],
      "metadata": {
        "id": "Sm9kX6HHyAyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVr9tNl2ca1M"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-readers-web\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-program-openai\n",
        "!pip install llama-index-agent-openai\n",
        "!pip install InstructorEmbedding\n",
        "!pip install llama-index-vector-stores-qdrant qdrant_client\n",
        "!pip install fastembed\n",
        "!pip install -q streamlit\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get .py file from github"
      ],
      "metadata": {
        "id": "lrZazRtta__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/CJacobsen1193/VHPteam3/main/streamlit.py"
      ],
      "metadata": {
        "id": "60nTFfcfaJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads our source data"
      ],
      "metadata": {
        "id": "6yGhfkHyf5fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnQeOfxeN3g"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets personal huggingface token for model access"
      ],
      "metadata": {
        "id": "ly4x1Umhf9gk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftOD5Bq-evbW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads LLama 2 7B Chat HuggingFace model"
      ],
      "metadata": {
        "id": "8xtQdIHUgAwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCHk9piHe3M6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    #bnb_4bit_quant_type=\"nf4\",\n",
        "    #bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "    context_window=3900,\n",
        "    model_kwargs={\"token\": hf_token, \"quantization_config\": quantization_config},\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index and embed source data to the model"
      ],
      "metadata": {
        "id": "m8Hw2rhMgGkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5DONLhQfnvU"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model='local:hkunlp/instructor-large'\n",
        "embeddings = Settings.embed_model(documents)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores embedded data to a VectorStore"
      ],
      "metadata": {
        "id": "VIlawb0KgKw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qcuYpPEibkC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.indices import SummaryIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "summary_index = SummaryIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests querying the model"
      ],
      "metadata": {
        "id": "zC00eEqZgmG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfSrBbKIjRp7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n",
        "response = query_engine.query(\"Does Franklin have veteran tuition benefits?\")\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores the indexed data to a QDrant cluster"
      ],
      "metadata": {
        "id": "vILJyPq2xrxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url='https://1d752ae2-4e0f-4101-ae0f-b59cd212e480.us-east4-0.gcp.cloud.qdrant.io',\n",
        "    api_key=\"ZEUHVnqv9sKXF1gHpY3u1pBKljE26BBoOqA2bkyAXKT7nEhCdq_xWA\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"mycollection\",\n",
        "    enable_hybrid=True,\n",
        "    batch_size=20\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=documents,\n",
        "    storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "wlEobc_klnrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from stored Qdrant database"
      ],
      "metadata": {
        "id": "D7E0_lu_xwLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model='local:hkunlp/instructor-large'\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://1d752ae2-4e0f-4101-ae0f-b59cd212e480.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=\"ZEUHVnqv9sKXF1gHpY3u1pBKljE26BBoOqA2bkyAXKT7nEhCdq_xWA\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"mycollection\", enable_hybrid=True)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "UDunp32RlqRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"context\", response_mode=\"simple_summarize\",\n",
        "                                   llm=llm, system_prompt=\"You are a chatbot, able to have normal interactions, as well as talk about Franklin University\")\n",
        "prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "response = chat_engine.chat(prompt)\n",
        "\n",
        "display_response(response)"
      ],
      "metadata": {
        "id": "gMgPrie6lsDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Streamlit\n",
        "\n",
        "when running last line of code copy the IP Adress, follow the url, and enter it into Tunnel Password"
      ],
      "metadata": {
        "id": "9fmrqUBajBdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "\n",
        "!npm install localtunnel\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXd2dOAcZkvN",
        "outputId": "cc813e57-bd26-493a-917d-7cab81bb2b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.407s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!streamlit run streamlit.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "QtZAXQneetWg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GHY5g6be5RK",
        "outputId": "87e3bdac-48bc-4372-fc9b-89843c3c80b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.124.210.218\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.971s\n",
            "your url is: https://strong-cities-invent.loca.lt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
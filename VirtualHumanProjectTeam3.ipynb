{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce8EJ-ahEjOX"
      },
      "source": [
        "To query the chatbot please follow these steps:\n",
        "1. Install all neccessary requirements\n",
        "2. Get personal huggingface token\n",
        "3. Load Llaama 2 7B Chat HuggingFace model\n",
        "4. Read from stored Qdrant database\n",
        "5. Query the database!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm9kX6HHyAyD"
      },
      "source": [
        "To run Streamlit:\n",
        "\n",
        "Install requirements below\n",
        "\n",
        "Get .py file from github\n",
        "\n",
        "Run codeblocks under \"Run Streamlit\" at end of notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVr9tNl2ca1M"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install llama-index-readers-web\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-program-openai\n",
        "!pip install llama-index-agent-openai\n",
        "!pip install InstructorEmbedding\n",
        "!pip install llama-index-vector-stores-qdrant qdrant_client\n",
        "!pip install fastembed\n",
        "!pip install -q streamlit\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrZazRtta__Z"
      },
      "source": [
        "Get .py file from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "60nTFfcfaJ61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c47cbe-ba9e-48ce-a949-e8e0b31fd474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-02 19:04:03--  https://raw.githubusercontent.com/CJacobsen1193/VHPteam3/main/streamlit.py?token=GHSAT0AAAAAACQTFXYOKRHOLGXDKDXSCDVUZRT4PKQ\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3192 (3.1K) [text/plain]\n",
            "Saving to: ‘streamlit.py’\n",
            "\n",
            "streamlit.py        100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-02 19:04:03 (49.2 MB/s) - ‘streamlit.py’ saved [3192/3192]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/CJacobsen1193/VHPteam3/main/streamlit.py?token=GHSAT0AAAAAACQTFXYOKRHOLGXDKDXSCDVUZRT4PKQ -O streamlit.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yGhfkHyf5fc"
      },
      "source": [
        "Loads our source data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnQeOfxeN3g"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly4x1Umhf9gk"
      },
      "source": [
        "Gets personal huggingface token for model access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftOD5Bq-evbW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xtQdIHUgAwG"
      },
      "source": [
        "Loads LLama 2 7B Chat HuggingFace model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCHk9piHe3M6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "    context_window=3900,\n",
        "    model_kwargs={\"token\": hf_token, \"quantization_config\": quantization_config},\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Hw2rhMgGkD"
      },
      "source": [
        "Index and embed source data to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5DONLhQfnvU"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model='local:hkunlp/instructor-large'\n",
        "#embeddings = Settings.embed_model(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qcuYpPEibkC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.indices import SummaryIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "summary_index = SummaryIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC00eEqZgmG2"
      },
      "source": [
        "Tests querying the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfSrBbKIjRp7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n",
        "response = query_engine.query(\"Does Franklin have veteran tuition benefits?\")\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vILJyPq2xrxk"
      },
      "source": [
        "Stores the indexed data to a QDrant cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlEobc_klnrE"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url='https://bbf8aa70-cd1d-4b4d-9f1f-dd4dbbfe0661.europe-west3-0.gcp.cloud.qdrant.io',\n",
        "    api_key=\"DrtpCMsJSmnBHHm5YFK5cEu0qFnaLjKVnmaSP-__Lx3cZqaGY0DaHw\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"mycollection\",\n",
        "    enable_hybrid=True,\n",
        "    batch_size=20\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=documents,\n",
        "    storage_context=storage_context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7E0_lu_xwLg"
      },
      "source": [
        "Read from stored Qdrant database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDunp32RlqRB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://097d6aff-312a-41fe-90e7-90219bf4a194.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=\"zeQHBdKQ5eZcgopVPI7uNVmisVDMJ4waGlfHjeAEU801klh-b35cIw\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"mycollection\", enable_hybrid=True)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMgPrie6lsDN"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"context\", response_mode=\"simple_summarize\",\n",
        "                                   llm=llm, system_prompt=\"You are a chatbot, able to have normal interactions, as well as talk about Franklin University\")\n",
        "prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "response = chat_engine.chat(prompt)\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fmrqUBajBdZ"
      },
      "source": [
        "Run Streamlit\n",
        "\n",
        "Run the codeblocks below in order\n",
        "\n",
        "When running last line of code copy the IP Adress, follow the url, and enter it into Tunnel Password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iXd2dOAcZkvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d991c9-f5d4-4cfe-d18d-80b46d54874d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.086s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QtZAXQneetWg"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!streamlit run /content/streamlit.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5GHY5g6be5RK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59900768-c486-43b1-c5b7-83eed73f4779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.240.158.120\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.136s\n",
            "your url is: https://smooth-lies-lead.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

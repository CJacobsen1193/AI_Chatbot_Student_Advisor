{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To query the chatbot please follow these steps:\n",
        "1. Install all neccessary requirements\n",
        "2. Get personal huggingface token\n",
        "3. Load Llaama 2 7B Chat HuggingFace model\n",
        "4. Read from stored Qdrant database\n",
        "5. Query the model!"
      ],
      "metadata": {
        "id": "Ce8EJ-ahEjOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs all necessary requirements"
      ],
      "metadata": {
        "id": "Nk_a6nStf2Sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVr9tNl2ca1M"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install llama-index-readers-web\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-program-openai\n",
        "!pip install llama-index-agent-openai\n",
        "!pip install InstructorEmbedding\n",
        "!pip install llama-index-vector-stores-qdrant qdrant_client\n",
        "!pip install fastembed\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads our source data"
      ],
      "metadata": {
        "id": "6yGhfkHyf5fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnQeOfxeN3g"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gets personal huggingface token for model access"
      ],
      "metadata": {
        "id": "ly4x1Umhf9gk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ftOD5Bq-evbW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads LLama 2 7B Chat HuggingFace model"
      ],
      "metadata": {
        "id": "8xtQdIHUgAwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCHk9piHe3M6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "    context_window=3900,\n",
        "    model_kwargs={\"token\": hf_token, \"quantization_config\": quantization_config},\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Index and embed source data to the model"
      ],
      "metadata": {
        "id": "m8Hw2rhMgGkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5DONLhQfnvU"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model='local:hkunlp/instructor-large'\n",
        "embeddings = Settings.embed_model(documents)\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores embedded data to a VectorStore"
      ],
      "metadata": {
        "id": "VIlawb0KgKw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qcuYpPEibkC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.indices import SummaryIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_documents(documents)\n",
        "summary_index = SummaryIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests querying the model"
      ],
      "metadata": {
        "id": "zC00eEqZgmG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfSrBbKIjRp7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n",
        "response = query_engine.query(\"Does Franklin have veteran tuition benefits?\")\n",
        "\n",
        "display_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores the indexed data to a QDrant cluster"
      ],
      "metadata": {
        "id": "vILJyPq2xrxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url='https://1d752ae2-4e0f-4101-ae0f-b59cd212e480.us-east4-0.gcp.cloud.qdrant.io',\n",
        "    api_key=\"ZEUHVnqv9sKXF1gHpY3u1pBKljE26BBoOqA2bkyAXKT7nEhCdq_xWA\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"mycollection\",\n",
        "    enable_hybrid=True,\n",
        "    batch_size=20\n",
        ")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=documents,\n",
        "    storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "wlEobc_klnrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from stored Qdrant database"
      ],
      "metadata": {
        "id": "D7E0_lu_xwLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "from qdrant_client import QdrantClient\n",
        "from llama_index.core import Settings\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model='local:hkunlp/instructor-large'\n",
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://1d752ae2-4e0f-4101-ae0f-b59cd212e480.us-east4-0.gcp.cloud.qdrant.io\",\n",
        "    api_key=\"ZEUHVnqv9sKXF1gHpY3u1pBKljE26BBoOqA2bkyAXKT7nEhCdq_xWA\",\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"mycollection\", enable_hybrid=True)\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "UDunp32RlqRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.notebook_utils import display_response\n",
        "\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"context\", response_mode=\"compact\")\n",
        "prompt = str(input(\"Ask me a question about Franklin University!  \"))\n",
        "response = chat_engine.chat(prompt)\n",
        "\n",
        "display_response(response)"
      ],
      "metadata": {
        "id": "gMgPrie6lsDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "cdf04c71-db7b-413c-d3cc-3b86c89aac28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask me a question about Franklin University!  How long does it take to complete a master's program in Computer Science at Franklin University?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**`Final Response:`** According to the information provided on the Franklin University website, the Master of Science in Computer Science program at Franklin University can be completed in as little as 12 months (1 year) of full-time study. However, the program can also be completed on a part-time basis, which can take up to 24 months (2 years) to complete."
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}